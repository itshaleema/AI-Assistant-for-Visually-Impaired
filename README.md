# AI-Assistant-for-Visually-Impaired
#Haleema Iftikhar
Contact: haleema4work@gmail.com
üîçSummary: Real-Time Object and Emotion Detection System for Assistive Vision Applications
Author: Haleema Iftikhar
Project Title: Real-Time Visual Perception System for Visually Impaired Users Using Deep Learning and On-Device Audio Feedback
Technologies: OpenCV, ONNX Runtime, pyttsx3, MobileNet-SSD, FER+, Haar cascades
Objectives:
1) Detect and classify everyday objects in the user‚Äôs environment.

2)Estimate the direction and relative distance of detected objects.

3)Recognize human emotions based on facial expressions.

4)Provide natural language feedback through real-time text-to-speech (TTS).

5)Operate in real time and requires minimal compute resources.
System Architecture & Methodology:
‚Ä¢	Object Detection:
The system uses a pre-trained MobileNet-SSD model trained on the COCO dataset to detect 80 common object classes. Detected objects are analyzed based on bounding box geometry to infer:
Direction (left, center, right) based on x-coordinate center
Distance (very close, medium, far) based on bounding box height
‚Ä¢	Face & Emotion Detection:
When a "person" is detected, the system performs:
Face localization using Haar cascades
Grayscale normalization and resizing to 64√ó64 resolution

Inference using the FER+ ONNX model for 8 emotion categories:
['neutral','happiness','surprise','sadness','anger','disgust','fear','contempt']
Emotion is only reported if its confidence probability exceeds a defined threshold (default: 40%).
‚Ä¢	Speech Output:
Pyttsx3 is used for text-to-speech conversion. Spoken messages are throttled using a temporal buffer to avoid information overload.
Features:
üì¶ Object Localization with verbal direction & proximity

üòä Emotion Recognition for social awareness

üß† ONNX Inference for hardware-efficient deep learning

üîä Voice Feedback to aid non-visual navigation

‚ö° Real-Time Performance on CPU-only systems

Applications:
‚Ä¢	Assistive technology for blind or low-vision individuals

‚Ä¢	Human‚Äìrobot interaction systems requiring emotion understanding

‚Ä¢	Context-aware wearable devices

‚Ä¢	Ambient intelligence in smart home or public environments

Limitations & Future Improvements:
‚Ä¢	Emotion recognition is limited by FER+ model accuracy and lighting conditions.

‚Ä¢	The current system supports a single-face emotion analysis.

‚Ä¢	Future versions may include:

‚Ä¢	Multilingual speech support

‚Ä¢	Depth estimation via stereo or IR

‚Ä¢	Gesture or action recognition

‚Ä¢	Integration with GPS or haptic feedback
