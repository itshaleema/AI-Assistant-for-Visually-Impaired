# Real-Time Object and Emotion Detection System for Assistive Vision Applications
### ğŸ‘©â€ğŸ’» Author: Haleema Iftikhar  
ğŸ“§ **Contact:** haleema4work@gmail.com  

---

## ğŸ” Summary
**Project Title:** *Real-Time Visual Perception System for Visually Impaired Users Using Deep Learning and On-Device Audio Feedback*  
**Technologies Used:** OpenCV, ONNX Runtime, pyttsx3, MobileNet-SSD, FER+, Haar Cascades  

---

## ğŸ¯ Objectives
1. Detect and classify everyday objects in the userâ€™s environment  
2. Estimate the **direction** and **relative distance** of detected objects  
3. Recognize **human emotions** based on facial expressions  
4. Provide **natural language feedback** using real-time text-to-speech (TTS)  
5. Operate in **real time** with minimal compute resources  

---

## ğŸ›  System Architecture & Methodology

### â€¢ Object Detection
- Uses **MobileNet-SSD** trained on the COCO dataset (80 object classes)
- Computes:
  - **Direction**: left, center, right (based on x-center of bounding box)
  - **Distance**: very close, medium, far (based on bounding box height)

### â€¢ Face & Emotion Detection
- Triggered when a `"person"` is detected
- Pipeline:
  - Face localized using **Haar Cascades**
  - Grayscale normalized and resized to **64Ã—64**
  - Inference via **FER+ ONNX** model
- Supported emotion categories:
  - `['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt']`
- Emotion is only reported if confidence > **40%**

### â€¢ Speech Output
- Uses **pyttsx3** for offline TTS
- Voice feedback is throttled to avoid cognitive overload

---

## ğŸŒŸ Key Features
- ğŸ“¦ **Object Localization** with verbal direction & proximity
- ğŸ˜Š **Emotion Recognition** for social interaction
- ğŸ§  **ONNX Runtime** for efficient on-device inference
- ğŸ”Š **Real-Time Voice Feedback** for non-visual navigation
- âš¡ Runs efficiently on **CPU-only systems**

---

## ğŸ§© Applications
- Assistive tech for **blind or visually impaired users**
- Emotion-aware **humanâ€“robot interaction**
- **Smart wearable devices** with contextual awareness
- **Ambient intelligence** in smart homes or public spaces

---

## âš ï¸ Limitations & Future Work
- Emotion recognition depends on FER+ accuracy and lighting
- Currently supports **only one face** at a time
- Future enhancements:
  - ğŸŒ **Multilingual TTS**
  - ğŸ•¹ï¸ **Gesture or action recognition**
  - ğŸ§­ **GPS or haptic feedback integration**
  - ğŸ‘“ Support for **depth sensors** (IR/stereo vision)

---
Thank you!!
